{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import src.constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define a function to extract value and unit from entity_value\n",
    "def extract_value_and_unit(entity_value):\n",
    "    if pd.isna(entity_value):  # Handle missing values\n",
    "        return None, None\n",
    "    match = re.match(r'([0-9.]+)\\s*(\\w+)', entity_value)\n",
    "    if match:\n",
    "        value = float(match.group(1))  # Extract the float value\n",
    "        unit = match.group(2).strip().lower()  # Extract and normalize the unit\n",
    "        return value, unit\n",
    "    return None, None  # In case the value isn't in the expected format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions for images and data\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here is another preprocess to may improve model performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url):\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "def extract_value_and_unit(entity_value):\n",
    "    if pd.isna(entity_value):  # Handle missing values\n",
    "        return None, None\n",
    "    match = re.match(r'([0-9.]+)\\s*(\\w+)', entity_value)\n",
    "    if match:\n",
    "        value = float(match.group(1))  # Extract the float value\n",
    "        unit = match.group(2).strip().lower()  # Extract and normalize the unit\n",
    "        return value, unit\n",
    "    return None, None  # In case the value isn't in the expected format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### here is the updated EntityValuePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityValuePredictor(nn.Module):\n",
    "    def __init__(self, num_entity_types):\n",
    "        super(EntityValuePredictor, self).__init__()\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.fc1 = nn.Linear(in_features, 1024)  \n",
    "        self.fc2 = nn.Linear(1024, 512)         \n",
    "        self.fc3 = nn.Linear(512, num_entity_types) \n",
    "        self.value_head = nn.Linear(512, 1)     \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        entity_type_logits = self.fc3(x)\n",
    "        value = self.value_head(x)\n",
    "        return entity_type_logits, value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### here is different train_model function for optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Updated training function\n",
    "def train_model(num_epochs, model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels, entity_types) in enumerate(tqdm(train_loader)):\n",
    "            inputs, labels, entity_types = inputs.cuda(), labels.cuda(), entity_types.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            entity_type_logits, predicted_value = model(inputs)\n",
    "            \n",
    "            loss_entity_type = criterion(entity_type_logits, entity_types)\n",
    "            loss_value = criterion(predicted_value.squeeze(), labels)\n",
    "            loss = loss_entity_type + loss_value\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), 'entity_value_predictor.pth')\n",
    "    print(f\"Model saved at 'entity_value_predictor.pth'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "# class EntityValuePredictor(nn.Module):\n",
    "#     def __init__(self, num_entity_types):\n",
    "#         super(EntityValuePredictor, self).__init__()\n",
    "#         self.backbone = models.resnet50(pretrained=True)\n",
    "#         in_features = self.backbone.fc.in_features\n",
    "#         self.backbone.fc = nn.Identity()\n",
    "#         self.fc1 = nn.Linear(in_features, 512)\n",
    "#         self.fc2 = nn.Linear(512, num_entity_types)\n",
    "#         self.value_head = nn.Linear(512, 1)  # Predict a single value\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.backbone(x)\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         entity_type_logits = self.fc2(x)\n",
    "#         value = self.value_head(x)\n",
    "#         return entity_type_logits, value\n",
    "\n",
    "# Training Process\n",
    "def train_model(train_data_path, model, criterion_entity, criterion_value, optimizer, entity_types, epochs=3):\n",
    "    # Load the training data\n",
    "    train_data = pd.read_csv(train_data_path)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for index, row in tqdm(train_data.iterrows(), total=len(train_data)):\n",
    "            try:\n",
    "                # Get image and process it\n",
    "                image = download_image(row['image_link'])\n",
    "                image_tensor = preprocess(image)\n",
    "                \n",
    "                # Extract entity name, value, and unit\n",
    "                entity_name = row['entity_name']\n",
    "                entity_value, unit = extract_value_and_unit(row['entity_value'])\n",
    "                if entity_value is None:\n",
    "                    continue  # Skip invalid rows\n",
    "                \n",
    "                entity_type_idx = entity_types.index(entity_name)\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                entity_type_logits, predicted_value = model(image_tensor.unsqueeze(0))\n",
    "                \n",
    "                # Compute losses\n",
    "                loss_entity = criterion_entity(entity_type_logits, torch.tensor([entity_type_idx]))\n",
    "                loss_value = criterion_value(predicted_value, torch.tensor([entity_value]))\n",
    "                \n",
    "                loss = loss_entity + loss_value\n",
    "                \n",
    "                # Backward and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {index}: {str(e)}\")\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_data)}\")\n",
    "\n",
    "\n",
    "    torch.save(model.state_dict(), 'entity_value_predictor.pth')\n",
    "    print(f\"Model saved at 'entity_value_predictor.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Downloads\\python\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\python\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss functions, and optimizer\n",
    "entity_types = list(src.constants.entity_unit_map.keys())\n",
    "num_entity_types = len(entity_types)\n",
    "model = EntityValuePredictor(num_entity_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_entity = nn.CrossEntropyLoss()  # For entity type classification\n",
    "criterion_value = nn.MSELoss()  # For entity value regression\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Downloads\\python\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      " 49%|████▉     | 158/320 [02:17<18:05,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing row 157: HTTPSConnectionPool(host='m.media-amazon.com', port=443): Max retries exceeded with url: /images/I/71pKB7-3itL.jpg (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000022554832FD0>, 'Connection to m.media-amazon.com timed out. (connect timeout=None)'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 159/320 [02:38<29:31, 11.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing row 158: HTTPSConnectionPool(host='m.media-amazon.com', port=443): Max retries exceeded with url: /images/I/71PGzqxh-EL.jpg (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000225548301D0>, 'Connection to m.media-amazon.com timed out. (connect timeout=None)'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 160/320 [02:59<37:22, 14.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing row 159: HTTPSConnectionPool(host='m.media-amazon.com', port=443): Max retries exceeded with url: /images/I/71uqF3qqsuL.jpg (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000022554835590>, 'Connection to m.media-amazon.com timed out. (connect timeout=None)'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 161/320 [03:20<42:43, 16.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing row 160: HTTPSConnectionPool(host='m.media-amazon.com', port=443): Max retries exceeded with url: /images/I/61vdMgkasML.jpg (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000022554833DD0>, 'Connection to m.media-amazon.com timed out. (connect timeout=None)'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 214/320 [04:31<12:27,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing row 213: HTTPSConnectionPool(host='m.media-amazon.com', port=443): Max retries exceeded with url: /images/I/71XLo+kfY4S.jpg (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000022554834210>, 'Connection to m.media-amazon.com timed out. (connect timeout=None)'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [05:44<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 183282.22927762306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [03:52<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Loss: 178517.23054127226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [03:47<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Loss: 178746.96869110168\n",
      "Model saved at 'entity_value_predictor.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model using train.csv\n",
    "train_data_path = 'dataset/sample_train.csv'\n",
    "train_model(train_data_path, model, criterion_entity, criterion_value, optimizer, entity_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Downloads\\python\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\python\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_34584\\874046630.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))   # Load the trained weights\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.constants import entity_unit_map  # Assuming this contains your entity names and allowed units\n",
    "\n",
    "# Load the trained model\n",
    "def load_model(model_path):\n",
    "    entity_types = list(entity_unit_map.keys())\n",
    "    num_entity_types = len(entity_types)\n",
    "    \n",
    "    model = EntityValuePredictor(num_entity_types)  # Initialize the model\n",
    "    model.load_state_dict(torch.load(model_path))   # Load the trained weights\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model, entity_types\n",
    "\n",
    "# Example usage:\n",
    "MODEL_PATH = 'entity_value_predictor.pth'\n",
    "model, entity_types = load_model(MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url):\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "\n",
    "def predictor(image_link, group_id, entity_name, model, entity_types):\n",
    "    try:\n",
    "        image = download_image(image_link)\n",
    "        image_tensor = preprocess(image)\n",
    "        prediction = predict(model, image_tensor, entity_name, entity_types)\n",
    "        return prediction\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_link}: {str(e)}\")\n",
    "        return \"\"\n",
    "    \n",
    "    \n",
    "def predict(model, image_tensor, entity_name, entity_types):\n",
    "    with torch.no_grad():\n",
    "        entity_type_logits, predicted_value = model(image_tensor.unsqueeze(0))\n",
    "    \n",
    "    # Predict entity type\n",
    "    entity_type_probs = torch.softmax(entity_type_logits, dim=1)\n",
    "    predicted_entity_index = torch.argmax(entity_type_probs).item()\n",
    "    predicted_entity = entity_types[predicted_entity_index]\n",
    "    \n",
    "    # If the predicted entity type doesn't match, return empty string\n",
    "    if predicted_entity != entity_name:\n",
    "        return \"\"  # Skip if the model predicts a different entity type\n",
    "    \n",
    "    # Get the predicted value\n",
    "    predicted_value = predicted_value.item()\n",
    "    \n",
    "    # Choose the appropriate unit based on the entity type\n",
    "    unit = next(iter(entity_unit_map[entity_name]))  # Get the correct unit for the entity type\n",
    "    \n",
    "    # Format the prediction string as \"x unit\"\n",
    "    return f\"{predicted_value:.2f} {unit}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:37<00:00,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file generated: dataset/test_out.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Preprocess the images and predict entity values\n",
    "\n",
    "\n",
    "def process_test_data(test_data_path, output_file, model, entity_types):\n",
    "    # Load the test dataset\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    \n",
    "    # Make predictions\n",
    "    tqdm.pandas()\n",
    "    test_data['prediction'] = test_data.progress_apply(\n",
    "        lambda row: predictor(row['image_link'], row['group_id'], row['entity_name'], model, entity_types), axis=1)\n",
    "    \n",
    "    # Save predictions to output CSV file\n",
    "    test_data[['index', 'prediction']].to_csv(output_file, index=False)\n",
    "    print(f\"Output file generated: {output_file}\")\n",
    "\n",
    "# Example usage:\n",
    "TEST_DATA_PATH = 'dataset/sample_test.csv'\n",
    "OUTPUT_FILE = 'dataset/test_out.csv'\n",
    "process_test_data(TEST_DATA_PATH, OUTPUT_FILE, model, entity_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# here is the another version(VIT model )to increase the performance of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_34584\\760633800.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n",
      " 33%|███▎      | 29/88 [00:14<00:54,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing https://m.media-amazon.com/images/I/51BEuVR4ZzL.jpg: HTTPSConnectionPool(host='m.media-amazon.com', port=443): Max retries exceeded with url: /images/I/51BEuVR4ZzL.jpg (Caused by SSLError(SSLError(1, '[SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1002)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:38<00:00,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file generated: dataset/test_out.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import timm  # Library for Vision Transformer\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import src.constants\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EntityValuePredictor(nn.Module):\n",
    "    def __init__(self, num_entity_types):\n",
    "        super(EntityValuePredictor, self).__init__()\n",
    "        # Use Vision Transformer (ViT) model from timm\n",
    "        self.backbone = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "        in_features = self.backbone.get_classifier().in_features\n",
    "        self.backbone.reset_classifier(0)\n",
    "        self.fc1 = nn.Linear(in_features, 512)\n",
    "        self.fc2 = nn.Linear(512, num_entity_types)\n",
    "        self.value_head = nn.Linear(512, 1)  # Predict a single value\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        entity_type_logits = self.fc2(x)\n",
    "        value = self.value_head(x)\n",
    "        return entity_type_logits, value\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def download_image(url):\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "def load_model(model, model_path):\n",
    "    # Load the checkpoint directly (without 'model_state_dict')\n",
    "    checkpoint = torch.load(model_path)\n",
    "\n",
    "    # Get the model's current state dictionary\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # Filter out layers that don't match in size\n",
    "    pretrained_dict = {k: v for k, v in checkpoint.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
    "\n",
    "    # Update the current model's state_dict with matching keys\n",
    "    model_dict.update(pretrained_dict)\n",
    "\n",
    "    # Load the updated state_dict into the model\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def predict(model, image_tensor, entity_name, entity_types):\n",
    "    with torch.no_grad():\n",
    "        entity_type_logits, value = model(image_tensor.unsqueeze(0))\n",
    "    \n",
    "    entity_type_probs = torch.softmax(entity_type_logits, dim=1)\n",
    "    predicted_entity_index = torch.argmax(entity_type_probs).item()\n",
    "    predicted_entity = entity_types[predicted_entity_index]\n",
    "    \n",
    "    if predicted_entity != entity_name:\n",
    "        return \"\"  # Return empty string if predicted entity doesn't match\n",
    "    \n",
    "    predicted_value = value.item()\n",
    "    \n",
    "    # Choose appropriate unit based on entity type\n",
    "    unit = next(iter(src.constants.entity_unit_map[entity_name]))\n",
    "    \n",
    "    # Format the prediction string\n",
    "    return f\"{predicted_value:.2f} {unit}\"\n",
    "\n",
    "def predictor(image_link, group_id, entity_name, model, entity_types):\n",
    "    try:\n",
    "        image = download_image(image_link)\n",
    "        image_tensor = preprocess(image)\n",
    "        prediction = predict(model, image_tensor, entity_name, entity_types)\n",
    "        return prediction\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_link}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DATASET_FOLDER = 'dataset/'\n",
    "    MODEL_PATH = 'entity_value_predictor.pth'  # Path to your trained model\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = EntityValuePredictor(num_entity_types)\n",
    "    model = load_model(model, MODEL_PATH)\n",
    "    # Load test data\n",
    "    test = pd.read_csv(os.path.join(DATASET_FOLDER, 'sample_test.csv'))\n",
    "    \n",
    "    # Make predictions\n",
    "    tqdm.pandas()\n",
    "    test['prediction'] = test.progress_apply(\n",
    "        lambda row: predictor(row['image_link'], row['group_id'], row['entity_name'], model, entity_types), axis=1)\n",
    "    \n",
    "    # Save predictions\n",
    "    output_filename = os.path.join(DATASET_FOLDER, 'test_out.csv')\n",
    "    test[['index', 'prediction']].to_csv(output_filename, index=False)\n",
    "\n",
    "    print(f\"Output file generated: {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['backbone.conv1.weight', 'backbone.bn1.weight', 'backbone.bn1.bias', 'backbone.bn1.running_mean', 'backbone.bn1.running_var', 'backbone.bn1.num_batches_tracked', 'backbone.layer1.0.conv1.weight', 'backbone.layer1.0.bn1.weight', 'backbone.layer1.0.bn1.bias', 'backbone.layer1.0.bn1.running_mean', 'backbone.layer1.0.bn1.running_var', 'backbone.layer1.0.bn1.num_batches_tracked', 'backbone.layer1.0.conv2.weight', 'backbone.layer1.0.bn2.weight', 'backbone.layer1.0.bn2.bias', 'backbone.layer1.0.bn2.running_mean', 'backbone.layer1.0.bn2.running_var', 'backbone.layer1.0.bn2.num_batches_tracked', 'backbone.layer1.0.conv3.weight', 'backbone.layer1.0.bn3.weight', 'backbone.layer1.0.bn3.bias', 'backbone.layer1.0.bn3.running_mean', 'backbone.layer1.0.bn3.running_var', 'backbone.layer1.0.bn3.num_batches_tracked', 'backbone.layer1.0.downsample.0.weight', 'backbone.layer1.0.downsample.1.weight', 'backbone.layer1.0.downsample.1.bias', 'backbone.layer1.0.downsample.1.running_mean', 'backbone.layer1.0.downsample.1.running_var', 'backbone.layer1.0.downsample.1.num_batches_tracked', 'backbone.layer1.1.conv1.weight', 'backbone.layer1.1.bn1.weight', 'backbone.layer1.1.bn1.bias', 'backbone.layer1.1.bn1.running_mean', 'backbone.layer1.1.bn1.running_var', 'backbone.layer1.1.bn1.num_batches_tracked', 'backbone.layer1.1.conv2.weight', 'backbone.layer1.1.bn2.weight', 'backbone.layer1.1.bn2.bias', 'backbone.layer1.1.bn2.running_mean', 'backbone.layer1.1.bn2.running_var', 'backbone.layer1.1.bn2.num_batches_tracked', 'backbone.layer1.1.conv3.weight', 'backbone.layer1.1.bn3.weight', 'backbone.layer1.1.bn3.bias', 'backbone.layer1.1.bn3.running_mean', 'backbone.layer1.1.bn3.running_var', 'backbone.layer1.1.bn3.num_batches_tracked', 'backbone.layer1.2.conv1.weight', 'backbone.layer1.2.bn1.weight', 'backbone.layer1.2.bn1.bias', 'backbone.layer1.2.bn1.running_mean', 'backbone.layer1.2.bn1.running_var', 'backbone.layer1.2.bn1.num_batches_tracked', 'backbone.layer1.2.conv2.weight', 'backbone.layer1.2.bn2.weight', 'backbone.layer1.2.bn2.bias', 'backbone.layer1.2.bn2.running_mean', 'backbone.layer1.2.bn2.running_var', 'backbone.layer1.2.bn2.num_batches_tracked', 'backbone.layer1.2.conv3.weight', 'backbone.layer1.2.bn3.weight', 'backbone.layer1.2.bn3.bias', 'backbone.layer1.2.bn3.running_mean', 'backbone.layer1.2.bn3.running_var', 'backbone.layer1.2.bn3.num_batches_tracked', 'backbone.layer2.0.conv1.weight', 'backbone.layer2.0.bn1.weight', 'backbone.layer2.0.bn1.bias', 'backbone.layer2.0.bn1.running_mean', 'backbone.layer2.0.bn1.running_var', 'backbone.layer2.0.bn1.num_batches_tracked', 'backbone.layer2.0.conv2.weight', 'backbone.layer2.0.bn2.weight', 'backbone.layer2.0.bn2.bias', 'backbone.layer2.0.bn2.running_mean', 'backbone.layer2.0.bn2.running_var', 'backbone.layer2.0.bn2.num_batches_tracked', 'backbone.layer2.0.conv3.weight', 'backbone.layer2.0.bn3.weight', 'backbone.layer2.0.bn3.bias', 'backbone.layer2.0.bn3.running_mean', 'backbone.layer2.0.bn3.running_var', 'backbone.layer2.0.bn3.num_batches_tracked', 'backbone.layer2.0.downsample.0.weight', 'backbone.layer2.0.downsample.1.weight', 'backbone.layer2.0.downsample.1.bias', 'backbone.layer2.0.downsample.1.running_mean', 'backbone.layer2.0.downsample.1.running_var', 'backbone.layer2.0.downsample.1.num_batches_tracked', 'backbone.layer2.1.conv1.weight', 'backbone.layer2.1.bn1.weight', 'backbone.layer2.1.bn1.bias', 'backbone.layer2.1.bn1.running_mean', 'backbone.layer2.1.bn1.running_var', 'backbone.layer2.1.bn1.num_batches_tracked', 'backbone.layer2.1.conv2.weight', 'backbone.layer2.1.bn2.weight', 'backbone.layer2.1.bn2.bias', 'backbone.layer2.1.bn2.running_mean', 'backbone.layer2.1.bn2.running_var', 'backbone.layer2.1.bn2.num_batches_tracked', 'backbone.layer2.1.conv3.weight', 'backbone.layer2.1.bn3.weight', 'backbone.layer2.1.bn3.bias', 'backbone.layer2.1.bn3.running_mean', 'backbone.layer2.1.bn3.running_var', 'backbone.layer2.1.bn3.num_batches_tracked', 'backbone.layer2.2.conv1.weight', 'backbone.layer2.2.bn1.weight', 'backbone.layer2.2.bn1.bias', 'backbone.layer2.2.bn1.running_mean', 'backbone.layer2.2.bn1.running_var', 'backbone.layer2.2.bn1.num_batches_tracked', 'backbone.layer2.2.conv2.weight', 'backbone.layer2.2.bn2.weight', 'backbone.layer2.2.bn2.bias', 'backbone.layer2.2.bn2.running_mean', 'backbone.layer2.2.bn2.running_var', 'backbone.layer2.2.bn2.num_batches_tracked', 'backbone.layer2.2.conv3.weight', 'backbone.layer2.2.bn3.weight', 'backbone.layer2.2.bn3.bias', 'backbone.layer2.2.bn3.running_mean', 'backbone.layer2.2.bn3.running_var', 'backbone.layer2.2.bn3.num_batches_tracked', 'backbone.layer2.3.conv1.weight', 'backbone.layer2.3.bn1.weight', 'backbone.layer2.3.bn1.bias', 'backbone.layer2.3.bn1.running_mean', 'backbone.layer2.3.bn1.running_var', 'backbone.layer2.3.bn1.num_batches_tracked', 'backbone.layer2.3.conv2.weight', 'backbone.layer2.3.bn2.weight', 'backbone.layer2.3.bn2.bias', 'backbone.layer2.3.bn2.running_mean', 'backbone.layer2.3.bn2.running_var', 'backbone.layer2.3.bn2.num_batches_tracked', 'backbone.layer2.3.conv3.weight', 'backbone.layer2.3.bn3.weight', 'backbone.layer2.3.bn3.bias', 'backbone.layer2.3.bn3.running_mean', 'backbone.layer2.3.bn3.running_var', 'backbone.layer2.3.bn3.num_batches_tracked', 'backbone.layer3.0.conv1.weight', 'backbone.layer3.0.bn1.weight', 'backbone.layer3.0.bn1.bias', 'backbone.layer3.0.bn1.running_mean', 'backbone.layer3.0.bn1.running_var', 'backbone.layer3.0.bn1.num_batches_tracked', 'backbone.layer3.0.conv2.weight', 'backbone.layer3.0.bn2.weight', 'backbone.layer3.0.bn2.bias', 'backbone.layer3.0.bn2.running_mean', 'backbone.layer3.0.bn2.running_var', 'backbone.layer3.0.bn2.num_batches_tracked', 'backbone.layer3.0.conv3.weight', 'backbone.layer3.0.bn3.weight', 'backbone.layer3.0.bn3.bias', 'backbone.layer3.0.bn3.running_mean', 'backbone.layer3.0.bn3.running_var', 'backbone.layer3.0.bn3.num_batches_tracked', 'backbone.layer3.0.downsample.0.weight', 'backbone.layer3.0.downsample.1.weight', 'backbone.layer3.0.downsample.1.bias', 'backbone.layer3.0.downsample.1.running_mean', 'backbone.layer3.0.downsample.1.running_var', 'backbone.layer3.0.downsample.1.num_batches_tracked', 'backbone.layer3.1.conv1.weight', 'backbone.layer3.1.bn1.weight', 'backbone.layer3.1.bn1.bias', 'backbone.layer3.1.bn1.running_mean', 'backbone.layer3.1.bn1.running_var', 'backbone.layer3.1.bn1.num_batches_tracked', 'backbone.layer3.1.conv2.weight', 'backbone.layer3.1.bn2.weight', 'backbone.layer3.1.bn2.bias', 'backbone.layer3.1.bn2.running_mean', 'backbone.layer3.1.bn2.running_var', 'backbone.layer3.1.bn2.num_batches_tracked', 'backbone.layer3.1.conv3.weight', 'backbone.layer3.1.bn3.weight', 'backbone.layer3.1.bn3.bias', 'backbone.layer3.1.bn3.running_mean', 'backbone.layer3.1.bn3.running_var', 'backbone.layer3.1.bn3.num_batches_tracked', 'backbone.layer3.2.conv1.weight', 'backbone.layer3.2.bn1.weight', 'backbone.layer3.2.bn1.bias', 'backbone.layer3.2.bn1.running_mean', 'backbone.layer3.2.bn1.running_var', 'backbone.layer3.2.bn1.num_batches_tracked', 'backbone.layer3.2.conv2.weight', 'backbone.layer3.2.bn2.weight', 'backbone.layer3.2.bn2.bias', 'backbone.layer3.2.bn2.running_mean', 'backbone.layer3.2.bn2.running_var', 'backbone.layer3.2.bn2.num_batches_tracked', 'backbone.layer3.2.conv3.weight', 'backbone.layer3.2.bn3.weight', 'backbone.layer3.2.bn3.bias', 'backbone.layer3.2.bn3.running_mean', 'backbone.layer3.2.bn3.running_var', 'backbone.layer3.2.bn3.num_batches_tracked', 'backbone.layer3.3.conv1.weight', 'backbone.layer3.3.bn1.weight', 'backbone.layer3.3.bn1.bias', 'backbone.layer3.3.bn1.running_mean', 'backbone.layer3.3.bn1.running_var', 'backbone.layer3.3.bn1.num_batches_tracked', 'backbone.layer3.3.conv2.weight', 'backbone.layer3.3.bn2.weight', 'backbone.layer3.3.bn2.bias', 'backbone.layer3.3.bn2.running_mean', 'backbone.layer3.3.bn2.running_var', 'backbone.layer3.3.bn2.num_batches_tracked', 'backbone.layer3.3.conv3.weight', 'backbone.layer3.3.bn3.weight', 'backbone.layer3.3.bn3.bias', 'backbone.layer3.3.bn3.running_mean', 'backbone.layer3.3.bn3.running_var', 'backbone.layer3.3.bn3.num_batches_tracked', 'backbone.layer3.4.conv1.weight', 'backbone.layer3.4.bn1.weight', 'backbone.layer3.4.bn1.bias', 'backbone.layer3.4.bn1.running_mean', 'backbone.layer3.4.bn1.running_var', 'backbone.layer3.4.bn1.num_batches_tracked', 'backbone.layer3.4.conv2.weight', 'backbone.layer3.4.bn2.weight', 'backbone.layer3.4.bn2.bias', 'backbone.layer3.4.bn2.running_mean', 'backbone.layer3.4.bn2.running_var', 'backbone.layer3.4.bn2.num_batches_tracked', 'backbone.layer3.4.conv3.weight', 'backbone.layer3.4.bn3.weight', 'backbone.layer3.4.bn3.bias', 'backbone.layer3.4.bn3.running_mean', 'backbone.layer3.4.bn3.running_var', 'backbone.layer3.4.bn3.num_batches_tracked', 'backbone.layer3.5.conv1.weight', 'backbone.layer3.5.bn1.weight', 'backbone.layer3.5.bn1.bias', 'backbone.layer3.5.bn1.running_mean', 'backbone.layer3.5.bn1.running_var', 'backbone.layer3.5.bn1.num_batches_tracked', 'backbone.layer3.5.conv2.weight', 'backbone.layer3.5.bn2.weight', 'backbone.layer3.5.bn2.bias', 'backbone.layer3.5.bn2.running_mean', 'backbone.layer3.5.bn2.running_var', 'backbone.layer3.5.bn2.num_batches_tracked', 'backbone.layer3.5.conv3.weight', 'backbone.layer3.5.bn3.weight', 'backbone.layer3.5.bn3.bias', 'backbone.layer3.5.bn3.running_mean', 'backbone.layer3.5.bn3.running_var', 'backbone.layer3.5.bn3.num_batches_tracked', 'backbone.layer4.0.conv1.weight', 'backbone.layer4.0.bn1.weight', 'backbone.layer4.0.bn1.bias', 'backbone.layer4.0.bn1.running_mean', 'backbone.layer4.0.bn1.running_var', 'backbone.layer4.0.bn1.num_batches_tracked', 'backbone.layer4.0.conv2.weight', 'backbone.layer4.0.bn2.weight', 'backbone.layer4.0.bn2.bias', 'backbone.layer4.0.bn2.running_mean', 'backbone.layer4.0.bn2.running_var', 'backbone.layer4.0.bn2.num_batches_tracked', 'backbone.layer4.0.conv3.weight', 'backbone.layer4.0.bn3.weight', 'backbone.layer4.0.bn3.bias', 'backbone.layer4.0.bn3.running_mean', 'backbone.layer4.0.bn3.running_var', 'backbone.layer4.0.bn3.num_batches_tracked', 'backbone.layer4.0.downsample.0.weight', 'backbone.layer4.0.downsample.1.weight', 'backbone.layer4.0.downsample.1.bias', 'backbone.layer4.0.downsample.1.running_mean', 'backbone.layer4.0.downsample.1.running_var', 'backbone.layer4.0.downsample.1.num_batches_tracked', 'backbone.layer4.1.conv1.weight', 'backbone.layer4.1.bn1.weight', 'backbone.layer4.1.bn1.bias', 'backbone.layer4.1.bn1.running_mean', 'backbone.layer4.1.bn1.running_var', 'backbone.layer4.1.bn1.num_batches_tracked', 'backbone.layer4.1.conv2.weight', 'backbone.layer4.1.bn2.weight', 'backbone.layer4.1.bn2.bias', 'backbone.layer4.1.bn2.running_mean', 'backbone.layer4.1.bn2.running_var', 'backbone.layer4.1.bn2.num_batches_tracked', 'backbone.layer4.1.conv3.weight', 'backbone.layer4.1.bn3.weight', 'backbone.layer4.1.bn3.bias', 'backbone.layer4.1.bn3.running_mean', 'backbone.layer4.1.bn3.running_var', 'backbone.layer4.1.bn3.num_batches_tracked', 'backbone.layer4.2.conv1.weight', 'backbone.layer4.2.bn1.weight', 'backbone.layer4.2.bn1.bias', 'backbone.layer4.2.bn1.running_mean', 'backbone.layer4.2.bn1.running_var', 'backbone.layer4.2.bn1.num_batches_tracked', 'backbone.layer4.2.conv2.weight', 'backbone.layer4.2.bn2.weight', 'backbone.layer4.2.bn2.bias', 'backbone.layer4.2.bn2.running_mean', 'backbone.layer4.2.bn2.running_var', 'backbone.layer4.2.bn2.num_batches_tracked', 'backbone.layer4.2.conv3.weight', 'backbone.layer4.2.bn3.weight', 'backbone.layer4.2.bn3.bias', 'backbone.layer4.2.bn3.running_mean', 'backbone.layer4.2.bn3.running_var', 'backbone.layer4.2.bn3.num_batches_tracked', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'value_head.weight', 'value_head.bias'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_1536\\1924205442.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(MODEL_PATH)\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(MODEL_PATH)\n",
    "print(checkpoint.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The another approach (EfficientNet model)for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetTransformerModel(nn.Module):\n",
    "    def __init__(self, num_units, num_heads, hidden_dim, num_layers):\n",
    "        super(EfficientNetTransformerModel, self).__init__()\n",
    "        \n",
    "        # Load pretrained EfficientNet model\n",
    "        self.efficientnet = models.efficientnet_b0(pretrained=True)\n",
    "        \n",
    "        # Modify the classifier head\n",
    "        num_ftrs = self.efficientnet.classifier[1].in_features\n",
    "        self.efficientnet.classifier = nn.Identity()\n",
    "        \n",
    "        # Define transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Define embedding and output layers\n",
    "        self.embedding = nn.Linear(num_ftrs, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, num_units)\n",
    "\n",
    "    def forward(self, images, entities, tgt):\n",
    "        # Extract features from EfficientNet\n",
    "        features = self.efficientnet(images)\n",
    "        \n",
    "        # Apply linear transformation to the features\n",
    "        embedded_features = self.embedding(features)\n",
    "        \n",
    "        # Transformer expects input in shape (seq_len, batch, features)\n",
    "        embedded_features = embedded_features.unsqueeze(0)  # Add sequence dimension\n",
    "        tgt = tgt.unsqueeze(0)  # Add sequence dimension\n",
    "        \n",
    "        # Pass through transformer\n",
    "        transformer_output = self.transformer(embedded_features, tgt)\n",
    "        \n",
    "        # Final output layer\n",
    "        output = self.fc_out(transformer_output.squeeze(0))  # Remove sequence dimension\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### data prepareation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def load_image(image_path):\n",
    "    if image_path.startswith('http'):\n",
    "        response = requests.get(image_path)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "    return image\n",
    "\n",
    "def preprocess_data(data, le_entity, le_units):\n",
    "    images = []\n",
    "    entities = []\n",
    "    values = []\n",
    "    for _, row in data.iterrows():\n",
    "        image = load_image(row['image_link'])\n",
    "        images.append(image)\n",
    "        entities.append(le_entity.transform([row['entity_name']])[0])  # Encode entity_name\n",
    "        value_unit = row['entity_value'].split()[-1]  # Extract unit\n",
    "        values.append(le_units.transform([value_unit])[0])\n",
    "    \n",
    "    images_tensor = torch.stack(images)\n",
    "    entities_tensor = torch.tensor(entities)\n",
    "    values_tensor = torch.tensor(values)\n",
    "    return images_tensor, entities_tensor, values_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load Data and Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data = pd.read_csv('dataset/sample_train.csv')\n",
    "test_data = pd.read_csv('dataset/sample_test.csv')\n",
    "\n",
    "# Label encode entity names and units\n",
    "le_entity = LabelEncoder()\n",
    "train_data['entity_name_encoded'] = le_entity.fit_transform(train_data['entity_name'])\n",
    "\n",
    "allowed_units = sorted(train_data['entity_value'].str.extract(r'([a-zA-Z]+)$')[0].unique())\n",
    "le_units = LabelEncoder()\n",
    "le_units.fit(allowed_units)\n",
    "\n",
    "# Filter test data to only include known entity names\n",
    "test_data = test_data[test_data['entity_name'].isin(le_entity.classes_)]\n",
    "\n",
    "# Encode entity names in the test data\n",
    "test_data['entity_name_encoded'] = le_entity.transform(test_data['entity_name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "train_data = pd.read_csv('dataset/sample_train.csv')\n",
    "test_data = pd.read_csv('dataset/sample_test.csv')\n",
    "\n",
    "# Label encode entity names and units\n",
    "le_entity = LabelEncoder()\n",
    "train_data['entity_name_encoded'] = le_entity.fit_transform(train_data['entity_name'])\n",
    "allowed_units = sorted(train_data['entity_value'].str.extract(r'([a-zA-Z]+)$')[0].unique())\n",
    "le_units = LabelEncoder()\n",
    "le_units.fit(allowed_units)\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def load_image(image_path):\n",
    "    if image_path.startswith('http'):\n",
    "        response = requests.get(image_path)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "    return image\n",
    "\n",
    "def preprocess_data(data, le_entity=None, le_units=None):\n",
    "    images = []\n",
    "    entities = []\n",
    "    values = []\n",
    "    for _, row in data.iterrows():\n",
    "        image = load_image(row['image_link'])\n",
    "        images.append(image)\n",
    "        if le_entity:\n",
    "            entities.append(row['entity_name_encoded'])\n",
    "        if 'entity_value' in row:\n",
    "            values.append(row['entity_value'])\n",
    "    \n",
    "    images_tensor = torch.stack(images)\n",
    "    if le_entity:\n",
    "        entities_tensor = torch.tensor(entities)\n",
    "    else:\n",
    "        entities_tensor = torch.empty(0, dtype=torch.long)  # Placeholder for test data\n",
    "    \n",
    "    if le_units:\n",
    "        values_tensor = torch.tensor([le_units.transform([val.split()[-1]])[0] for val in values])\n",
    "    else:\n",
    "        values_tensor = torch.empty(0, dtype=torch.long)  # Placeholder for test data\n",
    "    \n",
    "    return images_tensor, entities_tensor, values_tensor\n",
    "\n",
    "# Prepare data\n",
    "images_train, entities_train, values_train = preprocess_data(train_data, le_entity, le_units)\n",
    "images_test, entities_test, _ = preprocess_data(test_data)  # Test data does not have entity_value\n",
    "\n",
    "# Split data for training and validation\n",
    "train_indices, val_indices = train_test_split(range(len(images_train)), test_size=0.2, random_state=42)\n",
    "images_train, images_val = images_train[train_indices], images_train[val_indices]\n",
    "entities_train, entities_val = entities_train[train_indices], entities_train[val_indices]\n",
    "values_train, values_val = values_train[train_indices], values_train[val_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Downloads\\python\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\Downloads\\python\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\Lenovo\\Downloads\\python\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "num_units = len(le_units.classes_)\n",
    "hidden_dim = 256\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "\n",
    "model = EfficientNetTransformerModel(num_units=num_units, num_heads=num_heads, hidden_dim=hidden_dim, num_layers=num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, images_train, entities_train, values_train, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images_train, entities_train, entities_train)  # Modify as needed\n",
    "        loss = criterion(outputs.view(-1, num_units), values_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "train(model, images_train, entities_train, values_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Evaluate and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, images_val, entities_val, values_val):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images_val, entities_val, entities_val)  # Modify as needed\n",
    "        _, predicted = torch.max(outputs.view(-1, num_units), 1)\n",
    "        accuracy = (predicted == values_val).float().mean()\n",
    "        print(f'Validation Accuracy: {accuracy.item()}')\n",
    "\n",
    "evaluate(model, images_val, entities_val, values_val)\n",
    "\n",
    "def predict(model, images_test, entities_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images_test, entities_test, entities_test)  # Modify as needed\n",
    "        _, predicted = torch.max(outputs.view(-1, num_units), 1)\n",
    "        predicted_units = le_units.inverse_transform(predicted.numpy())\n",
    "        return predicted_units\n",
    "\n",
    "predictions = predict(model, images_test, entities_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
